import numpy as np
import h5py
import matplotlib.pyplot as plt

import copy
%matplotlib inline

def init_pars(n_i, n_h, n_o):
  W1 = np.random.randn(n_h,n_i) * 0.01
  b1 = np.zeros((n_h,1),dtype='float')
  W2 = np.random.randn(n_o,n_h) * 0.01
  b2 = np.zeros((n_y,1),dtype='float')

  pars = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
  return pars

def init_pars_deep(layer_dims):
  pars = {}
  no_layers = len(layer_dims)

  for l in range(1, no_layers):
    pars['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01
    pars['b' + str(l)] = np.zeros((layer_dims[l],1))*0.01

  return pars

def linear_fwd(A, W, b):
  Z = np.dot(W, A) + b
  cache = (A, W, b)
  return Z, cache

def linear_activation_fwd(A_prev, W, b, active):
  if active == "sigmoid":
    Z, linear_cache = linear_fwd(A_prev, W, b)
    A, activation_cache = sigmoid(Z)
  elif active == "relu":
    Z, linear_cache = linear_fwd(A_prev, W, b)
    A, activation_cache = relu(Z)
  cache = (linear_cache, activation_cache)

  return A, cache

def L_model_fwd(I, pars):
  caches = []
  A = I
  L = len(pars)

  for l in range(1, L):
    A_prev = A 
    A, cache = linear_activation_fwd(A_prev, pars['W' + str(l)], pars['b' + str(l)], "relu")
    caches.append(cache)

  AL, cache = linear_activation_fwd(A, pars['W' + str(L)], pars['b' + str(L)], "sigmoid")
  caches.append(cache)

  return AL, caches

def cost(AL, O):
  m = O.shape[1]
  cost = -( np.dot(Y, (np.log(AL)).T) + np.dot( (np.ones((Y.shape[0],Y.shape[1]), dtype=int) - Y.astype(int)), (np.log(np.ones((Y.shape[0],Y.shape[1]), dtype=int) - AL)).T))/m
  cost = np.squeeze(cost)
  return cost

def linear_bkward(dZ, cache):    
    A_prev, W, b = cache
    m = A_prev.shape[1]
   
    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m
    dA_prev = np.dot(W.T,dZ)   
    
    return dA_prev, dW, db

def linear_activation_bkward(dA, cache, active):   
    linear_cache, activation_cache = cache    
    if active == "relu":    
        dZ = relu_bkward(dA,activation_cache)
        dA_prev, dW, db = linear_bkward(dZ, linear_cache)        
    elif active == "sigmoid":      
        dZ = sigmoid_bkward(dA, activation_cache)
        dA_prev, dW, db = linear_bkward(dZ, linear_cache)          
    
    return dA_prev, dW, db


def L_model_bkward(AL, O, caches):
    gradients = {}    
    L = len(caches)
    m = AL.shape[1]
    O = O.reshape(AL.shape) 
  
    dAL = - (np.divide(O, AL) - np.divide(1 - O, 1 - AL))      
  
    current_cache = caches[L-1]
    dA_prev_temp, dW_temp, db_temp = linear_activation_bkward(dAL, current_cache, "sigmoid")
    gradients["dA" + str(L-1)] = dA_prev_temp
    gradients["dW" + str(L)] = dW_temp
    gradients["db" + str(L)] = db_temp
 
    for l in reversed(range(L-1)):      
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_bkward(dA_prev_temp, current_cache, "relu")
        gradients["dA" + str(l)] = dA_prev_temp
        gradients["dW" + str(l + 1)] = dW_temp
        gradients["db" + str(l + 1)] = db_temp

    return gradients

def update_pars(params, gradients, learning_rate):
  pars = copy.deepcopy(params)
  L = len(pars)

  for l in range(L):
    pars["W" + str(l+1)] = pars["W" + str(l+1)] - learning_rate * gradients["dW" + str(l + 1)]
    pars["b" + str(l+1)] = pars["b" + str(l+1)] - learning_rate * gradients["db" + str(l + 1)]    
  
  return pars











  

    
